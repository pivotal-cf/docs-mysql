---
title: Upgrading MySQL on Service Instances AND Recovering from HA MySQL Upgrade Failures
owner: MySQL
---

<strong><%= modified_date %></strong>

This topic provides instructions for upgrading service instances from Percona 5.7 plans to Percona 8.0 plans, and help with troubleshooting upgrade failures on HA instances.

## <a id="overview"></a>MySQL HA Upgrade Overview

To upgrade an existing 5.7 service instance to MySQL 8.0, you will update the service instance's plan to a different plan which specifies MySQL 8.0 in its definition. (The only way to upgrade an existing MySQL service instance from 5.7 to 8.0 is by updating it to a new service plan.)

1. Consult your platform operator or platform configuration to identify a suitable 8.0-enabled service plan to update your existing 5.7 service instance to.

We recommend keeping the plan topology consistent when upgrading MySQL from 5.7 to 8.0. For example, if you have a 5.7 single-node service instance and want to upgrade it to an 8.0 high-availability instance, first perform "cf update-service" on your service instance and designate a 8.0 single-instance plan. Then, once that update is successful and your service instance is running MySQL 8.0, perform a second "cf update-service" to change its topology from single-node to high-availability.

MySQL Tiles supporting both MySQL 5.7 and MySQL 8.0 offer double the number of service plans compared to previous 5.7-only tiles. Platform Operators choose to create equivalent MySQL 8.0 plans for each previously-existing MySQL 5.7 plan -- holding other plan factors constant while just adjusting the MySQL version in that plan. This plan configuration approach lets Platform Operators offer Developers plans that are identical except for the MySQL version, thereby facilitating the safe upgrade approach outlined in the previous paragraph.

2. [Backup your service instance](./backup-restore.html.md.erb) There is no downgrade path from 8.0 to 5.7. A backup preserves your ability to recover from any failure by restoring your data to a new 5.7 service instance if necessary.

3. Do not attempt the upgrade if you are experiencing any database problems or issues. For HA instances [check the health of your cluster](monitor-health.html.md.erb), and do not perform an upgrade unless all 3 cluster nodes are reporting as healthy. HA service instances attempt to continually serve queries and connections throughout the upgrade process.

4. Once the above are done, upgrade your service instances by following the instructions for [Updating all service instances](./upgrade.html.md.erb) or to [Upgrade an Individual Service Instance](./use.html##upgrade-an-individual-service-instance-12).

## =========================================

## <a id="recovering"></a>Recovering from MySQL HA Upgrade Failures

### <a id="overview"></a>Recovery Overview

When running "cf update-service" and failures happen usually there is an underlying node problem. In order to make progess 
with the update it is necessary to find and fix the underlying issue with the node.
A common issue can be a full disk which prevents the service from starting. See the [Toubleshooting Node Upgrade Failures](#troubleshooting-node-upgrade-failures) for more information.  
Once the underlying node issue has been addressed run the same  "cf update-service" command as previously.  
If this does not resolve the issue then manually update each node that has not yet been updated as described in section [Manually Upgrade Cluster](#manually-upgrade-cluster)

## <a id="manually-upgrade-cluster"></a>Manually upgrade cluster

To "bosh deploy" any remaining 5.7 instances we start with the instance outside of the cluster
(to preserve the cluster's 2-node minimum).use

1. Target your broken node by instructing BOSH to ignore the other nodes in your cluster.
   For each node except the bootstrap node you identified above, run the bosh command with ignore.  
   For example, if you are updating instance 0 you will want to ignore 1 and 2:

         ```
         bosh -e YOUR-ENV -d YOUR-DEPLOYMENT ignore mysql/1
         bosh -e YOUR-ENV -d YOUR-DEPLOYMENT ignore mysql/2
         ```

1. Turn off the BOSH Resurrector by running:

      ```
      bosh update-resurrection off
      ```

1. Use the BOSH manifest to update the instance to force the version to 8.0.  First you will export the manifest into a yaml file to edit with the following command:
      ```
      bosh -e YOUR-ENV -d YOUR-DEPLOYMENT manifest > /tmp/manifest.yml
      ```
    Using a text editor, manually modify the `mysql_version` property from "5.7" to "8.0". The full path of the property is `/instance_groups/name=mysql/jobs/name=pxc-mysql/properties/mysql_version?`. Then use the following command to deploy the newly updated manifest to your targeted instance VM.
      ```
      bosh -e YOUR-ENV -d YOUR-DEPLOYMENT deploy /tmp/manifest.yml
      ```

1. (At this stage you may want to check that all 3 nodes are in you cluster.)

1. If you started this process with an 8.0 cluster, then all 3 nodes should now be at 8.0; perform the below cleanup steps.use

If you started this process with a mixed 8.0 & 5.7 cluster, repeat the above process for the remaining 5.7 node, being careful to
"bosh unignore" that remaining node and "bosh ignore" the node you just recovered.to

1. At this stage, all your nodes should be running 8.0 and should be in the cluster.

1. Unignore any remaining nodes you ignored above:
  For example, if you had previously ignored instances 1 and 2, you would run:
         ```
         bosh -e YOUR-ENV -d YOUR-DEPLOYMENT unignore mysql/1
         bosh -e YOUR-ENV -d YOUR-DEPLOYMENT unignore mysql/2
         ```

1. Turn BOSH resurrector back on by running:
      ```
      bosh update-resurrection on
      ```
## <a id="troubleshooting-node-upgrade-failures"></a>Troubleshooting Node Upgrade Failures

Here are observations on what might cause a node to fail in an upgrade, and things to look at before
proceeding with the above steps ("bosh deploy" / "cf update-service" etc.)

1. An error on one MySQL VM can trigger upgrade errors on other nodes. Check the health of all nodes
(including disk usage), not just the failing node.

In a test where mysql/2's disk was filled prior to upgrading the cluster, mysql/0 was be the first node to fail during the upgrade process.
That mysql/0 upgraded its bits to 8.0, found the cluster, and tried to join: (from mysql/0 mysql.err.log)

> [System] [MY-013381] [Server] Server upgrade from '50700' to '80032' started.
> [System] [MY-013381] [Server] Server upgrade from '50700' to '80032' completed.
> ...
> [Note] [MY-000000] [WSREP] Server status change connected -> joiner
> ...
> [Note] [MY-000000] [Galera] Member 2.0 (mysql/0) requested state transfer from '*any*'. Selected 1.0 (mysql/2)(SYNCED) as donor.

In this case mysql/2 was malfunctioning; the state transfer to mysql/0 appears to been interrupted and/or improperly terminated:
(Still from mysql/0 mysql.err.log)

> [Note] [MY-000000] [Galera] 1.0 (mysql/2): State transfer to 2.0 (mysql/0) complete.
> [Note] [MY-000000] [Galera] Member 1.0 (mysql/2) synced with group.
> ...
> [ERROR] [MY-000000] [WSREP-SST] ******************* FATAL ERROR **********************
> [ERROR] [MY-000000] [WSREP-SST] Possible timeout in receving first data from donor in gtid/keyring stage
> [ERROR] [MY-000000] [WSREP-SST] Line 1381
> [ERROR] [MY-000000] [WSREP-SST] ******************************************************
> [ERROR] [MY-000000] [WSREP-SST] Cleanup after exit with status:32
> [ERROR] [MY-000000] [WSREP] Process completed with error: wsrep_sst_xtrabackup-v2 ...(Broken pipe)
> [ERROR] [MY-000000] [WSREP] Failed to read uuid:seqno from joiner script.
> [ERROR] [MY-000000] [WSREP] SST script aborted with error 32 (Broken pipe)
> [Note] [MY-000000] [Galera] Processing SST received
> [Note] [MY-000000] [Galera] SST request was cancelled
> [ERROR] [MY-000000] [Galera] State transfer request failed unrecoverably: 32 (Broken pipe). Most likely it is due to inability to communicate with the cluster primary component. Restart required.

During the above, here's the view from mysql/2 as it tried and failed to fulfill mysql/0's state transfer request:
(from mysql/2 mysql.err.log)

> [Note] WSREP: Member 2.0 (mysql/0) requested state transfer from '*any*'. Selected 1.0 (mysql/2)(SYNCED) as donor.
> [Note] WSREP: (aa7d5ebb, 'ssl://0.0.0.0:4567') turning message relay requesting off
> [Note] WSREP: monitor wait failed for causal read (repl.causal_read_timeout): : 110 (Connection timed out)
           at /var/vcap/data/compile/percona-xtradb-cluster-5.7/Percona-XtraDB-Cluster-5.7.41-31.65/percona-xtradb-cluster-galera/galerautils/src/gu_lock.hpp:wait(
  ):97
>[Note] WSREP: monitor wait failed for causal read (repl.causal_read_timeout): : 110 (Connection timed out)
           at /var/vcap/data/compile/percona-xtradb-cluster-5.7/Percona-XtraDB-Cluster-5.7.41-31.65/percona-xtradb-cluster-galera/galerautils/src/gu_lock.hpp:wait(
  ):97

Further down, the logs identify the underlying root cause: (from mysql/2 mysql.err.log)

> [ERROR] Disk is full writing './mysql-bin.000001' (Errcode: -1891240384 - No space left on device). Waiting for someone to free space...
> [ERROR] Retry in 60 secs. Message reprinted in 600 secs
> [Note] WSREP: monitor wait failed for causal read (repl.causal_read_timeout): : 110 (Connection timed out)
           at /var/vcap/data/compile/percona-xtradb-cluster-5.7/Percona-XtraDB-Cluster-5.7.41-31.65/percona-xtradb-cluster-galera/galerautils/src/gu_lock.hpp:wait():97
> [Note] WSREP: monitor wait failed for causal read (repl.causal_read_timeout): : 110 (Connection timed out)
           at /var/vcap/data/compile/percona-xtradb-cluster-5.7/Percona-XtraDB-Cluster-5.7.41-31.65/percona-xtradb-cluster-galera/galerautils/src/gu_lock.hpp:wait():97
> [Note] WSREP: monitor wait failed for causal read (repl.causal_read_timeout): : 110 (Connection timed out)
           at /var/vcap/data/compile/percona-xtradb-cluster-5.7/Percona-XtraDB-Cluster-5.7.41-31.65/percona-xtradb-cluster-galera/galerautils/src/gu_lock.hpp:wait():97

2.
